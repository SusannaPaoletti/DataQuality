{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQUA6SgMYqXk"
      },
      "source": [
        "# 0) LIBRARIES & DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXvoi_51eof6",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install ydata-profiling efficient-apriori recordlinkage\n",
        "!pip install outlier-utils\n",
        "!git clone https://github.com/SusannaPaoletti/DataQuality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpd6mSCOtq7_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from ydata_profiling import ProfileReport\n",
        "import recordlinkage\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvk-BwnBH9u1"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn import linear_model\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn import ensemble\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
        "\n",
        "from DataQuality import scripts_for_E5 as s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmEfCQrDo72c"
      },
      "outputs": [],
      "source": [
        "# load dirty dataset\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/SusannaPaoletti/DataQuality/refs/heads/main/Comune-di-Milano-Attivita-commerciali-di-media-e-grande-distribuzione.csv\", sep=';', encoding='utf-16')\n",
        "data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ-eYWGyt8yv"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Sy2N0h6t620"
      },
      "source": [
        "# 1) DATA EXPLORATION & PROFILING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3U51a1VhbR9Q"
      },
      "outputs": [],
      "source": [
        "#display dataset columns\n",
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xg3AmMMzbUV_"
      },
      "outputs": [],
      "source": [
        "#display the first 5 rows\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX_BsCXpu7Xn"
      },
      "outputs": [],
      "source": [
        "# display the shape of the dataset\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjox4_euvy8q"
      },
      "outputs": [],
      "source": [
        "#description of the numerical variables\n",
        "data.describe()\n",
        "#note that min(Superficie totale) = 0 which is clearly an error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MggYU8h0t9m6"
      },
      "outputs": [],
      "source": [
        "# display column types\n",
        "data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8DOrBS9hzNJ"
      },
      "outputs": [],
      "source": [
        "# number of distinct values of Tipo Via\n",
        "data['Tipo Via'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttEbQzLQhdUH"
      },
      "outputs": [],
      "source": [
        "# distinct values of Tipo Via\n",
        "data['Tipo Via'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rp_PV38OhoIt"
      },
      "outputs": [],
      "source": [
        "# number of distinct values of Settore Merceologico\n",
        "data['Settore Merceologico'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9j9NfQyhvwo"
      },
      "outputs": [],
      "source": [
        "# distinct values of Settore Merceologico\n",
        "data['Settore Merceologico'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVy3TZwEif-s"
      },
      "outputs": [],
      "source": [
        "# count occurrences for each unique value of Settore Merceologico\n",
        "settore_count = data['Settore Merceologico'].value_counts()\n",
        "settore_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cj57b1liGxN"
      },
      "outputs": [],
      "source": [
        "# count occurrences for each unique value of Insegna\n",
        "insegna_count = data['Insegna'].value_counts()\n",
        "insegna_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MqrK-U8zxwf"
      },
      "outputs": [],
      "source": [
        "# distinct values of ZD\n",
        "data['ZD'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ7wMY-qjF9E"
      },
      "source": [
        "DUPLICATES:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4uKiJYgvTX2"
      },
      "outputs": [],
      "source": [
        "# display exact duplicates\n",
        "duplicates = data.duplicated()\n",
        "print(\"There's\", duplicates.sum(), \"exact duplicate:\")\n",
        "data[data.duplicated()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAFQ8NNzi8pZ"
      },
      "outputs": [],
      "source": [
        "# percentage of duplicate rows in the dataset\n",
        "data.duplicated().sum()/data.shape[0]*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQl7CFhrkCEh"
      },
      "source": [
        "NULL VALUES:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMJTsSF8jfXX"
      },
      "outputs": [],
      "source": [
        "# missing values in the dataset\n",
        "data.isnull()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8whm1EAkZH2"
      },
      "outputs": [],
      "source": [
        "# display null values per column\n",
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3k1CtctGkoSs"
      },
      "outputs": [],
      "source": [
        "# total number of null values\n",
        "NULL = data.isnull().sum().sum()\n",
        "NULL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccDQ2fH1kPHu"
      },
      "source": [
        "NOT NULL VALUES:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ym2XSWYUkgxb"
      },
      "outputs": [],
      "source": [
        "# display not null values per column\n",
        "data.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FORMILwLk6I5"
      },
      "outputs": [],
      "source": [
        "# total number of values in the dataset\n",
        "TOT = data.shape[0]*data.shape[1]\n",
        "TOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-ONq6kHw77Z"
      },
      "outputs": [],
      "source": [
        "# select numerical columns from the dataset\n",
        "NUM = list(data.select_dtypes(include=['int64','float64']).columns)\n",
        "DATA_NUM = data[NUM]\n",
        "DATA_NUM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNnwZdaAbrSU"
      },
      "outputs": [],
      "source": [
        "# check the correlation between numerical columns\n",
        "corr = DATA_NUM.corr(method ='pearson')\n",
        "sns.heatmap(corr)\n",
        "plt.show()\n",
        "# note: as we expected there's significant correlation between Superficie vendita, S. totale e S. altri usi but also between ZD and Codice Via"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BpTdlR9bvss"
      },
      "outputs": [],
      "source": [
        "# plot value distributions per each column\n",
        "DATA = data.select_dtypes(include=['int64','float64'])\n",
        "for col in DATA.columns:\n",
        "    print(\"Histogram for \"+col+\":\")\n",
        "    plt.figure(figsize=(20,6))\n",
        "\n",
        "    DATA[col].hist()\n",
        "    print(\"\\n\\n\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ut3aPc0cekpb"
      },
      "outputs": [],
      "source": [
        "# generate a profile report\n",
        "PROFILE = ProfileReport(data, title=\"Pandas Profiling Report\")\n",
        "PROFILE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ipXEWcQuSTd"
      },
      "source": [
        "# 2) DATA QUALITY ASSESSMENT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77paJGP4lS-t"
      },
      "source": [
        "COMPLETENESS EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NXndrPfutv8"
      },
      "outputs": [],
      "source": [
        "# calculate data completeness ratio\n",
        "COMPLETENESS = (TOT - NULL) / TOT\n",
        "COMPLETENESS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDDVuGZrmDNr"
      },
      "outputs": [],
      "source": [
        "# format completeness as a percentage with one decimal place\n",
        "COMPLETENESS = '{0:.1f}%'.format(COMPLETENESS*100)\n",
        "print(COMPLETENESS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyeHGVfumb0Y"
      },
      "source": [
        "ACCURACY EVALUATION:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S11UdaoBl7s7"
      },
      "outputs": [],
      "source": [
        "# define the range of incorrect values based on dataset documentation\n",
        "RANGE_INCORRECT = range(0, 250)\n",
        "\n",
        "# count entries in 'Superficie totale' that fall within the incorrect range\n",
        "INCORRECT_Superficie_Totale = sum(1 for item in data['Superficie totale'] if item in RANGE_INCORRECT)\n",
        "\n",
        "INCORRECT_Superficie_Totale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w61j0lowo-kO"
      },
      "outputs": [],
      "source": [
        "# count the number of non-null entries in 'Superficie totale'\n",
        "NOT_NULL_Superficie_totale= data['Superficie totale'].count()\n",
        "NOT_NULL_Superficie_totale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWTw3HstpBTl"
      },
      "outputs": [],
      "source": [
        "# calculate the accuracy for 'Superficie totale' based on incorrect and non-null values\n",
        "ACCURACY_SUPERFICIE_TOT = (data.shape[0] - INCORRECT_Superficie_Totale) / NOT_NULL_Superficie_totale\n",
        "\n",
        "# format accuracy as a percentage with one decimal place\n",
        "ACCURACY_SUPERFICIE_TOT = '{0:.1f}%'.format(ACCURACY_SUPERFICIE_TOT * 100)\n",
        "\n",
        "print(ACCURACY_SUPERFICIE_TOT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKN0g-wWpW6Q"
      },
      "source": [
        "CONSISTENCY EVALUATION:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMyHSE8VpWQ4"
      },
      "outputs": [],
      "source": [
        "# integrity rule: superficie vendita + superficie altri usi = superficie totale\n",
        "data['consistency'] = np.where(data['Superficie vendita'] + data['Superficie altri usi'].fillna(0) != data['Superficie totale'], 0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxnAow7WqB45"
      },
      "outputs": [],
      "source": [
        "# sum the values in the 'consistency' column\n",
        "CONSISTENT = data['consistency'].sum()\n",
        "CONSISTENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh71jCrBrL3N"
      },
      "outputs": [],
      "source": [
        "# count consistent rows\n",
        "COUNT = data['consistency'].count()\n",
        "COUNT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m773YqFfqDeZ"
      },
      "outputs": [],
      "source": [
        "# calculate consistency as the ratio of consistent values to total count\n",
        "CONSISTENCY = CONSISTENT / COUNT\n",
        "\n",
        "# format consistency as a percentage with one decimal place\n",
        "CONSISTENCY = '{0:.1f}%'.format(CONSISTENCY * 100)\n",
        "\n",
        "print(CONSISTENCY)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pBZ3YZnlr6L"
      },
      "outputs": [],
      "source": [
        "# drop the 'consistency' column from the dataset\n",
        "data = data.drop(columns='consistency')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCREVjoVs9Ul"
      },
      "source": [
        "TIMELINESS EVALUATION won't be considered because we don't have the temporal context of data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOQTYKearj-R"
      },
      "source": [
        "# 3) DATA CLEANING (DATA TRANSFORMATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILM7Tun_cBhM"
      },
      "outputs": [],
      "source": [
        "# convert 'Superficie altri usi' to type int\n",
        "data['Superficie altri usi'] = data['Superficie altri usi'].fillna(0).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNwauUserqtU"
      },
      "outputs": [],
      "source": [
        "# drop exact duplicates\n",
        "data = data.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPCLYVADzxWE"
      },
      "outputs": [],
      "source": [
        "# extracts unique values of 'Tipo Via'\n",
        "unique_values = data['Tipo Via'].dropna().unique()\n",
        "print(\"Valori univoci nella colonna 'Tipo via':\", unique_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G28Wv0TcyCGV"
      },
      "outputs": [],
      "source": [
        "# extracts new 'Tipo via' from 'Ubicazione'\n",
        "data[['Tipo via estratta', 'Primo split']] = data['Ubicazione'].str.split(\" \", n=1, expand=True)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wg1JHp4NyjUI"
      },
      "outputs": [],
      "source": [
        "# Valid 'Tipo Via' values\n",
        "tipo_via = ['PZA', 'RIP', 'VIA', 'VLE', 'PLE', 'CSO', 'GLL', 'FOR', 'LGO', 'LARGO', 'BST', 'PAS', 'PTA', 'ALZ']\n",
        "\n",
        "# Select rows with invalid 'Tipo Via' values\n",
        "non_valide = data[~data['Tipo via estratta'].isin(tipo_via)]\n",
        "\n",
        "# Remove invalid rows if any are found\n",
        "if not non_valide.empty:\n",
        "    print(\"Invalid tuples found:\")\n",
        "    print(non_valide[['Ubicazione', 'Tipo via estratta']])\n",
        "\n",
        "    data = data.drop(non_valide.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpbbH0Nlu52R"
      },
      "outputs": [],
      "source": [
        "# define column names for comparison\n",
        "colonna1 = 'Tipo Via'\n",
        "colonna2 = 'Tipo via estratta'\n",
        "\n",
        "# extract rows where values in the columns differ\n",
        "righe_differenti = data[data[colonna1] != data[colonna2]]\n",
        "\n",
        "# display rows with different values in the specified columns\n",
        "righe_differenti[[colonna1, colonna2]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8DmS5wNvJSO"
      },
      "outputs": [],
      "source": [
        "# drop old column 'Tipo Via'\n",
        "data = data.drop('Tipo Via', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W93EcQuYtTzm"
      },
      "outputs": [],
      "source": [
        "# extract new 'Via' values from 'Primo split' using a regular expression\n",
        "data['Via estratta'] = data['Primo split'].str.extract(r'^([A-Z\\s\\.\\']+?)(?=\\s(?:N\\.|\\sN |\\snum\\.|\\(|[a-z]))')\n",
        "\n",
        "# remove extracted part from 'Primo split' and clean up the string\n",
        "data['Primo split'] = data['Primo split'].str.replace(r'^([A-Z\\s\\.\\']+?)(?=\\s(?:N\\.|\\sN |\\snum\\.|\\(|[a-z]))', '', regex=True).str.strip()\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqCHhdqkzDsx"
      },
      "outputs": [],
      "source": [
        "# define column names for comparison\n",
        "colonna1 = 'Via'\n",
        "colonna2 = 'Via estratta'\n",
        "\n",
        "# extract rows where values in the columns differ\n",
        "righe_differenti = data[data[colonna1] != data[colonna2]]\n",
        "\n",
        "# display rows with different values in the specified columns\n",
        "righe_differenti[[colonna1, colonna2]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ug6oepczJ-n"
      },
      "outputs": [],
      "source": [
        "# drop old column 'Via'\n",
        "data = data.drop('Via', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ari3_c2Exey"
      },
      "outputs": [],
      "source": [
        "# extract new 'Civico' values from 'Primo split' using a regular expression\n",
        "data['Civico estratto'] = data['Primo split'].str.extract(r'(\\d[\\/\\w]*)(?=\\s|;)', expand=True)\n",
        "\n",
        "# remove leading zeros from the extracted 'Civico' values\n",
        "data['Civico estratto'] = data['Civico estratto'].str.replace(r'^0{1,2}', '', regex=True)\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2H7jk2GOzVt_"
      },
      "outputs": [],
      "source": [
        "# convert 'Civico' column to integer, filling NaN values with -1\n",
        "data['Civico'] = data['Civico'].fillna(-1).astype(int)\n",
        "\n",
        "# convert both 'Civico' and 'Civico estratto' columns to string type\n",
        "data['Civico'] = data['Civico'].astype(str)\n",
        "data['Civico estratto'] = data['Civico estratto'].astype(str)\n",
        "\n",
        "# extract rows where values in the columns differ\n",
        "righe_differenti = data[data['Civico'] != data['Civico estratto']]\n",
        "\n",
        "# display rows with different values in the specified columns\n",
        "righe_differenti[['Civico', 'Civico estratto']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSp-SQy72D8X"
      },
      "outputs": [],
      "source": [
        "# drop old calumn 'Civico'\n",
        "data = data.drop('Civico', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxrW2LHS-BUB"
      },
      "outputs": [],
      "source": [
        "# extract new 'ZD' values from 'Primo split' using a regular expression\n",
        "data['(z.d.) estratto'] = data['Primo split'].str.extract(r'\\(z\\.d\\.\\s*(\\d+)\\)', expand=True)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UcHO5iI2Qdw"
      },
      "outputs": [],
      "source": [
        "# convert '(z.d.) estratto' column to integer type\n",
        "data['(z.d.) estratto'] = data['(z.d.) estratto'].astype(int)\n",
        "\n",
        "# extract rows where values in the columns differ\n",
        "righe_differenti = data[data['ZD'] != data['(z.d.) estratto']]\n",
        "\n",
        "# display rows with different values in the specified columns\n",
        "righe_differenti[['ZD', '(z.d.) estratto']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJomMFOO2t4H"
      },
      "outputs": [],
      "source": [
        "# drop old columns\n",
        "data = data.drop('ZD', axis=1)\n",
        "data = data.drop('Ubicazione', axis=1)\n",
        "data = data.drop('Primo split', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOU1-Fdj-_Ly"
      },
      "outputs": [],
      "source": [
        "# rename new columns\n",
        "data.rename(columns={'Via estratta': 'Via', 'Civico estratto': 'Civico', '(z.d.) estratto': 'ZD', 'Tipo via estratta': 'Tipo Via',}, inplace=True)\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f6VhkKlrq7-"
      },
      "source": [
        "# 4) DATA CLEANING (ERROR DETECTION&CORRECTION MISSING VALUES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIS0kjDee4ot"
      },
      "outputs": [],
      "source": [
        "# Function to update 'Superficie altri usi' and fix inconsistencies with 'Superficie totale'\n",
        "def aggiorna_superficie(row):\n",
        "    if (row['Superficie altri usi']) == 0 and row['Superficie totale'] > row['Superficie vendita']:\n",
        "        # If 'Superficie altri usi' is 0 and 'Superficie totale' is greater than 'Superficie vendita'\n",
        "        row['Superficie altri usi'] = row['Superficie totale'] - row['Superficie vendita']  # Fill NaN values for 'Superficie altri usi'\n",
        "    elif (row['Superficie altri usi']) == 0 and row['Superficie totale'] == row['Superficie vendita']:\n",
        "        row['Superficie altri usi'] = 0  # Fill NaN values for 'Superficie altri usi'\n",
        "    elif (row['Superficie altri usi']) == 0 and row['Superficie totale'] < row['Superficie vendita']:\n",
        "        row['Superficie totale'] = row['Superficie vendita']  # Adjust 'Superficie totale' when it's less than 'Superficie vendita'\n",
        "        row['Superficie altri usi'] = 0  # Fill NaN values for 'Superficie altri usi'\n",
        "    elif (row['Superficie altri usi']) != 0:\n",
        "        row['Superficie totale'] = row['Superficie vendita'] + row['Superficie altri usi']  # Correct 'Superficie totale' if 'Superficie altri usi' is not 0\n",
        "    return row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsp5b38joTPI"
      },
      "outputs": [],
      "source": [
        "# apply the 'aggiorna_superficie' function to each row of the dataset\n",
        "data = data.apply(aggiorna_superficie, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_15xSrS9oeKF"
      },
      "outputs": [],
      "source": [
        "# print the updated data with changes to the 'Superficie totale' column\n",
        "print(\"Dati aggiornati con le modifiche alla colonna 'Superficie totale':\")\n",
        "data[['Superficie vendita', 'Superficie altri usi', 'Superficie totale']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtXIl9wChN2d"
      },
      "source": [
        "ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OaKwufzZNUe"
      },
      "outputs": [],
      "source": [
        "# display the number of missing values in each column\n",
        "data.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0pVBv0zRsCw"
      },
      "outputs": [],
      "source": [
        "# remove rows where both 'Settore Merceologico' and 'Insegna' are NaN\n",
        "data = data.dropna(subset=['Settore Merceologico', 'Insegna'], how='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ESIaVELqn5A"
      },
      "outputs": [],
      "source": [
        "# fill NaN values in the 'Insegna' column with 'Non Specificato'\n",
        "data['Insegna'] = data['Insegna'].fillna('Non Specificato')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBnRL9hNZrCd"
      },
      "outputs": [],
      "source": [
        "# save the updated dataset to a CSV file without including the index\n",
        "data.to_csv('dataset_aggiornato.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4w1gUv5Qrmx"
      },
      "outputs": [],
      "source": [
        "# print rows where 'Settore Merceologico' is NaN before imputation\n",
        "print(\"Prima dell'imputazione:\")\n",
        "data[data['Settore Merceologico'].isna()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adn76e9agYdF"
      },
      "source": [
        "Logistic Regression for categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3__9pmSiMu7O"
      },
      "outputs": [],
      "source": [
        "# rimozione delle colonne non rilevanti\n",
        "df = data.drop(['Codice Via', 'Tipo Via', 'Via', 'Civico', 'ZD'], axis=1)\n",
        "\n",
        "# separazione delle colonne categoriche e numeriche\n",
        "CAT = list(df.select_dtypes(include=['object']).columns)\n",
        "NUM = list(df.select_dtypes(include=['int64', 'float64']).columns)\n",
        "CAT.remove('Settore Merceologico')  # escludiamo la variabile target\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vvbfeYfM7_D"
      },
      "outputs": [],
      "source": [
        "# Create new columns with imputed values using standard techniques\n",
        "for feature in df.columns:\n",
        "    if feature == 'Settore Merceologico':\n",
        "        continue\n",
        "\n",
        "    # Initialize new column for imputed values\n",
        "    df[feature + '_imp'] = df[feature]\n",
        "\n",
        "    # For numerical features, impute with median\n",
        "    if feature in NUM:\n",
        "        df.loc[df[feature].isnull(), feature + '_imp'] = df[feature].median()\n",
        "\n",
        "    # For categorical features, impute with mode\n",
        "    elif feature in CAT:\n",
        "        df.loc[df[feature].isnull(), feature + '_imp'] = df[feature].mode()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEBXTNoeM542"
      },
      "outputs": [],
      "source": [
        "# Impute missing values using Machine Learning\n",
        "IMP_DATA = df.copy()\n",
        "\n",
        "for feature in ['Settore Merceologico']:  # Target to predict\n",
        "    IMP_DATA['IMP_' + feature] = df[feature]\n",
        "    parameters = list(set(df.columns) - {'Settore Merceologico', 'IMP_' + feature})\n",
        "\n",
        "    # Encode categorical variables\n",
        "    X = s.encoding_categorical_variables(df[parameters])\n",
        "\n",
        "    # Filter valid and missing rows\n",
        "    valid_rows = df[feature].notnull()\n",
        "    missing_rows = df[feature].isnull()\n",
        "\n",
        "    # Logistic regression model\n",
        "    model = linear_model.LogisticRegression(max_iter=1000)\n",
        "    model.fit(X.loc[valid_rows], df[feature][valid_rows])\n",
        "\n",
        "    # Predict missing values\n",
        "    predictions = model.predict(X.loc[missing_rows])\n",
        "\n",
        "    # Replace missing values with predictions\n",
        "    IMP_DATA.loc[missing_rows, 'IMP_' + feature] = predictions\n",
        "\n",
        "# Filter and print rows where the 'Settore Merceologico' was imputed\n",
        "modified_rows = IMP_DATA[IMP_DATA['Settore Merceologico'].isnull() & IMP_DATA['IMP_Settore Merceologico'].notnull()]\n",
        "\n",
        "modified_rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHpRza0HS80X"
      },
      "outputs": [],
      "source": [
        "# print the first 10 predicted values for the imputed 'Settore Merceologico'\n",
        "print(predictions[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXdF0QGDgQGc"
      },
      "source": [
        "Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpnOFjBfYazm"
      },
      "outputs": [],
      "source": [
        "# Remove irrelevant columns from the dataset\n",
        "df = data.drop(['Codice Via', 'Tipo Via', 'Via', 'Civico', 'ZD'], axis=1)\n",
        "\n",
        "# Separate categorical and numerical columns\n",
        "CAT = list(df.select_dtypes(include=['object']).columns)\n",
        "NUM = list(df.select_dtypes(include=['int64', 'float64']).columns)\n",
        "\n",
        "# Simple Imputation (using mode for categorical variables)\n",
        "for feature in ['Settore Merceologico']: # Changed\n",
        "\n",
        "    df[feature + '_imp'] = df[feature]\n",
        "\n",
        "    # For numerical features, impute with median\n",
        "    if feature in NUM:\n",
        "        df.loc[df[feature].isnull(), feature + '_imp'] = df[feature].median()\n",
        "\n",
        "    # For categorical features, impute with mode\n",
        "    if feature in CAT:\n",
        "        df.loc[df[feature].isnull(), feature + '_imp'] = df[feature].mode()[0]\n",
        "\n",
        "# Imputation with Random Forest\n",
        "IMP_DATA = df.copy()\n",
        "\n",
        "for feature in ['Settore Merceologico']:  # Target to predict\n",
        "    IMP_DATA['IMP_' + feature] = df[feature]\n",
        "    parameters = list(set(df.columns) -  {feature + '_imp'} - {'Settore Merceologico', 'IMP_' + feature})\n",
        "\n",
        "    # Encode categorical variables\n",
        "    X = s.encoding_categorical_variables(df[parameters])\n",
        "\n",
        "    # Filter valid and missing rows\n",
        "    valid_rows = df[feature].notnull()\n",
        "    missing_rows = df[feature].isnull()\n",
        "\n",
        "    # Random Forest model for imputation\n",
        "    if feature in NUM:\n",
        "        model = ensemble.RandomForestRegressor()\n",
        "    elif feature in CAT:\n",
        "        model = ensemble.RandomForestClassifier()\n",
        "\n",
        "    # Train the model on non-null data\n",
        "    model.fit(X.loc[valid_rows], df[feature][valid_rows])\n",
        "\n",
        "    # Predict missing values\n",
        "    predictions = model.predict(X.loc[missing_rows])\n",
        "\n",
        "    # Replace missing values with predictions\n",
        "    IMP_DATA.loc[missing_rows, 'IMP_' + feature] = predictions\n",
        "\n",
        "# Print the rows that were modified\n",
        "modified_rows = IMP_DATA[IMP_DATA['Settore Merceologico'].isnull() & IMP_DATA['IMP_Settore Merceologico'].notnull()]\n",
        "modified_rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSHbL8unUEGJ"
      },
      "outputs": [],
      "source": [
        "# substitute the imputed data inside the main dataset\n",
        "data['Settore Merceologico'] = IMP_DATA['IMP_Settore Merceologico']\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUn0jlYxCHOv"
      },
      "outputs": [],
      "source": [
        "# Count the total number of null values in the dataset\n",
        "NULL = data.isnull().sum().sum()\n",
        "NULL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx1pqhYbr2f0"
      },
      "source": [
        "# 5) DATA CLEANING (ERROR DETECTION&CORRECTION OUTLIERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zIBgF7muyhu"
      },
      "outputs": [],
      "source": [
        "# Identify and fix wrong values for Civico\n",
        "# Note : we certainly know that the only invalid civico is '0' based on how we extracted it from 'Ubicazione'\n",
        "def drop_invalid_civico(df):\n",
        "    # Identify rows where Civico is '0'\n",
        "    invalid_rows = df[df['Civico'].astype(str) == \"0\"]\n",
        "\n",
        "    # Print invalid rows if any are found\n",
        "    if not invalid_rows.empty:\n",
        "        print(\"Rows where 'Civico' is '0':\")\n",
        "        print(invalid_rows)\n",
        "    else:\n",
        "        print(\"No rows found with 'Civico' equal to '0'.\")\n",
        "\n",
        "    # Drop rows with Civico = '0' and return the updated DataFrame\n",
        "    return df[df['Civico'].astype(str) != \"0\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqhnDrTDwNnx"
      },
      "outputs": [],
      "source": [
        "# Drop rows with invalid Civico\n",
        "data=drop_invalid_civico(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijp1Ep9xo2kX"
      },
      "outputs": [],
      "source": [
        "# Check for negative values in surface columns\n",
        "colonne_superfici = ['Superficie totale', 'Superficie vendita', 'Superficie altri usi']\n",
        "\n",
        "for colonna in colonne_superfici:\n",
        "    valori_negativi = data[data[colonna] < 0]\n",
        "    if not valori_negativi.empty:\n",
        "        print(f\"Negative values found in column '{colonna}':\")\n",
        "        print(valori_negativi)\n",
        "    else:\n",
        "        print(f\"No negative values in column '{colonna}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBvq9bCe8uin"
      },
      "outputs": [],
      "source": [
        "# Plot histogram for 'Superficie totale'\n",
        "plt.figure(figsize=(10, 6))  # Set the figure size\n",
        "\n",
        "# Seaborn histogram\n",
        "sns.distplot(data[\"Superficie totale\"], hist=True, kde=False, bins=int(180/5), color = 'blue', hist_kws={'edgecolor':'black'})\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Distribution of Superficie totale', fontsize=16)\n",
        "plt.xlabel('Superficie totale', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "\n",
        "# Show grid for better readability\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaHDMJuo-PAV"
      },
      "source": [
        "STATISTIC-BASED:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf8V4gYbFqyv"
      },
      "source": [
        "Z-Score e STD are not reliable since these parametric methods assume the values follow a Normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUfbqV4GFGjd"
      },
      "outputs": [],
      "source": [
        "# Function to detect outliers using Z-score method\n",
        "def ZS(data, threshold):\n",
        "    mean = np.mean(data)\n",
        "    sd = np.std(data)\n",
        "    outliers = []\n",
        "    for i in data:\n",
        "        z = (i - mean) / sd\n",
        "        if abs(z) > threshold:\n",
        "            outliers.append(i)\n",
        "    return outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIShdnpeFN_e"
      },
      "outputs": [],
      "source": [
        "# Apply ZS function to detect outliers in 'Superficie totale' column with threshold of 3\n",
        "ZS(data['Superficie totale'], 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuC3Z-ipFnQy"
      },
      "source": [
        "Standard Deviation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5d5gnA8FycA"
      },
      "outputs": [],
      "source": [
        "# Function to detect outliers using standard deviation method\n",
        "def STD(data, th):\n",
        "    mean = data.mean()\n",
        "    std = data.std()\n",
        "    V1 = mean + th * std\n",
        "    V2 = mean - th * std\n",
        "    outliers = []\n",
        "    outliers_ind = []\n",
        "    for d in data:\n",
        "        if (d > V1) | (d < V2):\n",
        "            outliers.append(d)\n",
        "\n",
        "    return outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Um0whUk4Fw-P"
      },
      "outputs": [],
      "source": [
        "# Apply STD function to detect outliers in 'Superficie totale' column with threshold of 3\n",
        "STD(data['Superficie totale'], 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_6VC4pkGmkG"
      },
      "outputs": [],
      "source": [
        "# Function to detect outliers using Robust Z-score based on Median and Median Absolute Deviation (MAD)\n",
        "def ZSB(data, threshold):\n",
        "    # Robust Zscore as a function of median and median\n",
        "    # median absolute deviation (MAD) defined as\n",
        "    # z-score = |x â€“ median(x)| / mad(x)\n",
        "    median = np.median(data)\n",
        "    print(\"Median: \", median)\n",
        "    median_absolute_deviation = np.median(np.abs(data - median))\n",
        "    modified_z_scores = (data - median) / median_absolute_deviation\n",
        "    outliers = data[np.abs(modified_z_scores) > threshold]\n",
        "\n",
        "    print(\"The detected outliers are: \", str(outliers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDAG4JDoGpTs"
      },
      "outputs": [],
      "source": [
        "ZSB(data['Superficie totale'], 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsTSYT-QF9oK"
      },
      "outputs": [],
      "source": [
        "# Function to detect outliers using percentiles method (1st and 99th percentile)\n",
        "def PERC(data):\n",
        "    V1 = np.percentile(data, 99)\n",
        "    V2 = np.percentile(data , 1)\n",
        "    outliers = []\n",
        "    for d in data:\n",
        "        if (d > V1) | (d < V2):\n",
        "            outliers.append(d)\n",
        "    return outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zU3uubjF_ua"
      },
      "outputs": [],
      "source": [
        "# Apply PERC function to detect outliers in 'Superficie totale' column\n",
        "PERC(data['Superficie totale'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ektor2f593od"
      },
      "source": [
        "IQR (non parametric approach)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3L2-okt92aa"
      },
      "outputs": [],
      "source": [
        "def IQR(data):\n",
        "    sorted(data)\n",
        "    Q1, Q3 = np.percentile(data, [25, 75])\n",
        "    IQR = Q3 - Q1\n",
        "    lower_range = Q1 - (2 * IQR)\n",
        "    upper_range = Q3 + (4 * IQR) # increased parameter since the value distribution is skewed\n",
        "    outliers = data[((data < lower_range) | (data > upper_range))]\n",
        "    # print outliers\n",
        "    print(\"The detected outliers are: \", str(outliers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDexG_t0_AAh"
      },
      "outputs": [],
      "source": [
        "IQR(data['Superficie totale'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP87UG9qHIYB"
      },
      "source": [
        "MODEL-BASED: KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UOLkEoIJe9T"
      },
      "outputs": [],
      "source": [
        "# Creating a new dataframe with only the surface data\n",
        "data_superfici = data.loc[:, ['Superficie vendita', 'Superficie altri usi', 'Superficie totale']]\n",
        "# show the structure of the new dataframe\n",
        "print(data_superfici.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eR64hTxZHM_u"
      },
      "outputs": [],
      "source": [
        "#Training\n",
        "X = data_superfici.values\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNQGa2XBHa4A"
      },
      "outputs": [],
      "source": [
        "#Instantiate model (KNN)\n",
        "KNN = NearestNeighbors(n_neighbors = 3) #finds the 3 nearest neighbors for each sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbIWCq5DHcb5"
      },
      "outputs": [],
      "source": [
        "#Fit model\n",
        "KNN.fit(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zruNWK6dHeXP"
      },
      "outputs": [],
      "source": [
        "#Distances and indexes of k-neaighbors from model outputs\n",
        "distances, indexes = KNN.kneighbors(X)\n",
        "distances.mean(axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCtZSwtXHgJl"
      },
      "outputs": [],
      "source": [
        "#Plot mean of k-distances of each observation\n",
        "plt.plot(distances.mean(axis =1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gGAF1U6MaAA"
      },
      "outputs": [],
      "source": [
        "global_mean_dist=np.mean(distances.mean(axis =1))\n",
        "print(global_mean_dist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8A9mvX9WHiGP"
      },
      "outputs": [],
      "source": [
        "# Exclude points at a high distance\n",
        "# threshold: cutoff values > 95th percentile of the mean distance (since value distribution is skewed)\n",
        "OUTLIER_INDEX = np.where(distances.mean(axis = 1) > np.percentile(distances.mean(axis =1), 95))\n",
        "OUTLIER_INDEX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T53jrO5AHjyW"
      },
      "outputs": [],
      "source": [
        "#Filter outlier values\n",
        "OUTLIERS_VALUES = data_superfici.iloc[OUTLIER_INDEX]\n",
        "OUTLIERS_VALUES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYvFJjkvHlZ8"
      },
      "outputs": [],
      "source": [
        "#Plot outliers\n",
        "plt.scatter(data_superfici['Superficie totale'], data_superfici['Superficie vendita'], color = \"royalblue\")\n",
        "plt.scatter(OUTLIERS_VALUES['Superficie totale'], OUTLIERS_VALUES['Superficie vendita'], color = \"hotpink\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlxugA42r8mS"
      },
      "source": [
        "# 6) DATA CLEANING (DATA DEDUPLICATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPQCGZNhorR0"
      },
      "source": [
        "**1. drop exact duplicates**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFvn0v-2oO_B"
      },
      "outputs": [],
      "source": [
        "# show exact duplicates\n",
        "data[data.duplicated()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmjmzHKpoXI3"
      },
      "outputs": [],
      "source": [
        "# drop duplicates\n",
        "data = data.drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0E2diWsou3f"
      },
      "source": [
        "**2. search for candidate duplicates with Record Linkage**\n",
        "\n",
        "method 1: indexing with Blocking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSY4waR6oube"
      },
      "outputs": [],
      "source": [
        "#create a copy of the dataset\n",
        "data1=data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ia6nIDa4oqEA"
      },
      "outputs": [],
      "source": [
        "#load the recordlinkage.Index class\n",
        "indexer = recordlinkage.Index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wiR2j_EpFTN"
      },
      "outputs": [],
      "source": [
        "# definition of the blocking criteria by choosing the attributes who discriminate more\n",
        "indexer.block(['Tipo Via', 'Via', 'Civico'])\n",
        "candidate_links=indexer.index(data1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdzbR-h0p9AU"
      },
      "outputs": [],
      "source": [
        "# show the comparisons found\n",
        "print(len(candidate_links))\n",
        "candidate_links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMoCv_Owp82N"
      },
      "outputs": [],
      "source": [
        "# definition of the rules to compare the candidate pairs\n",
        "compare_cl = recordlinkage.Compare()\n",
        "compare_cl.exact('ZD', 'ZD', label='ZD')\n",
        "compare_cl.exact('Codice Via', 'Codice Via', label='Codice Via')\n",
        "compare_cl.string('Insegna', 'Insegna', method='jarowinkler', threshold=0.80, label='Insegna')\n",
        "compare_cl.numeric('Superficie vendita', 'Superficie vendita', method='linear', offset=15, label='Superficie vendita')\n",
        "compare_cl.numeric('Superficie altri usi', 'Superficie altri usi', method='linear', offset=15, label='Superficie altri usi')\n",
        "compare_cl.numeric('Superficie totale', 'Superficie totale', method='linear', offset=30, label='Superficie totale')\n",
        "features = compare_cl.compute(candidate_links, data1)\n",
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCIAJ-Ottqhd"
      },
      "outputs": [],
      "source": [
        "# Count and sort the number of record pairs by their total similarity score\n",
        "features.sum(axis=1).value_counts().sort_index(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeV5NtFet3_E"
      },
      "outputs": [],
      "source": [
        "# Compare the matches\n",
        "matches = features[features.sum(axis=1) >4]\n",
        "\n",
        "print(len(matches))\n",
        "\n",
        "matches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2LxPeFzpPWk"
      },
      "source": [
        "qui dobbiamo decidere quali righe droppare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3alIWp97yti9"
      },
      "outputs": [],
      "source": [
        "# display candidate duplicates\n",
        "data.loc[[12, 20]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emboa2rPdleK"
      },
      "outputs": [],
      "source": [
        "# Drop line 12\n",
        "# We retained duplicate 20 because the 'Settore Merceologico' value is more general\n",
        "data = data.drop([12])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vry-ISlnXWa8"
      },
      "outputs": [],
      "source": [
        "# display candidate duplicates\n",
        "data.loc[[363, 364]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89L5u-HTOWri"
      },
      "outputs": [],
      "source": [
        "# Calculate the mean of \"Superficie vendita\" and \"Superficie totale\" for indices 363 and 364\n",
        "mean_superficie_vendita = data.loc[[363, 364], \"Superficie vendita\"].mean()\n",
        "mean_superficie_totale = data.loc[[363, 364], \"Superficie totale\"].mean()\n",
        "\n",
        "# Update the row 363 with the mean values\n",
        "data.loc[363, \"Superficie vendita\"] = math.ceil(mean_superficie_vendita)\n",
        "data.loc[363, \"Superficie totale\"] = math.ceil(mean_superficie_totale)\n",
        "\n",
        "# Drop the duplicate row (index 364)\n",
        "data = data.drop(index=364)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adsbYkzURKa-"
      },
      "outputs": [],
      "source": [
        "data.loc[[363]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bwbmm9QwVFdV"
      },
      "outputs": [],
      "source": [
        "# display candidate duplicates\n",
        "data.loc[[299, 727]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nj04c4zXeJST"
      },
      "outputs": [],
      "source": [
        "# Drop line 299\n",
        "# We retained duplicate 727 because is more specific on the 'Insegna' value\n",
        "data = data.drop([299])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOt2TycVWnrO"
      },
      "outputs": [],
      "source": [
        "data.loc[[821, 612]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5zpX3CwePmU"
      },
      "outputs": [],
      "source": [
        "# Drop line 612\n",
        "# We retained duplicate 821 because it aligns with the most recent 'Insegna' name displayed on the external source Google Maps (accessed on January 10, 2025)\n",
        "data = data.drop([612])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_trCJuZjVLlY"
      },
      "outputs": [],
      "source": [
        "# display candidate duplicates\n",
        "data.loc[[875, 874]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWoKSmNleheV"
      },
      "outputs": [],
      "source": [
        "# Calculate the mean of \"Superficie vendita\" and \"Superficie totale\" for indices 875 and 874\n",
        "mean_superficie_vendita = data.loc[[875, 874], \"Superficie vendita\"].mean()\n",
        "mean_superficie_totale = data.loc[[875, 874], \"Superficie totale\"].mean()\n",
        "\n",
        "# Update the row 875 with the mean values\n",
        "data.loc[875, \"Superficie vendita\"] = math.ceil(mean_superficie_vendita)\n",
        "data.loc[875, \"Superficie totale\"] = math.ceil(mean_superficie_totale)\n",
        "\n",
        "# Drop the duplicate row (index 874)\n",
        "data = data.drop(index=874)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2VrajahS20L"
      },
      "outputs": [],
      "source": [
        "data.loc[[875]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2809DAOj3UvA"
      },
      "outputs": [],
      "source": [
        "# display candidate duplicates\n",
        "data.loc[[609, 608]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DW7Fv7nNe36A"
      },
      "outputs": [],
      "source": [
        "# Drop line 608\n",
        "# We retained duplicate 609 because it aligns with the most recent 'Insegna' name displayed on the external source Google Maps (accessed on January 10, 2025)\n",
        "data = data.drop([608])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQylVvL-TuX6"
      },
      "outputs": [],
      "source": [
        "# display candidate duplicates\n",
        "data.loc[[692, 691]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWRKm1DGf5AX"
      },
      "outputs": [],
      "source": [
        "# Drop line 692.\n",
        "#We retained duplicate 691 because it aligns with the most recent 'Insegna' name displayed on the external source Google Maps (accessed on January 10, 2025)\n",
        "data = data.drop([692])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_kktQ43Xq5Z"
      },
      "source": [
        "method 2: indexing with Sorted Neighborhood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4uCztWyXvq2"
      },
      "outputs": [],
      "source": [
        "# Sort the record in the database on the basis of a key (Attribute/attributes)\n",
        "data1['Key'] = data['Tipo Via'] + data['Via'] + data['Civico']\n",
        "\n",
        "indexer = recordlinkage.index.SortedNeighbourhood(\n",
        "        on='Key', window=5 #Increasing the length of the window, the number of candidate links increases as well\n",
        "    )\n",
        "\n",
        "candidate_links = indexer.index(data1)\n",
        "print(len(candidate_links))\n",
        "candidate_links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvOf3wocX9Ak"
      },
      "outputs": [],
      "source": [
        "# definition of the rules to compare the candidate pairs\n",
        "compare_cl = recordlinkage.Compare()\n",
        "\n",
        "compare_cl.exact('Via', 'Via', label='Via')\n",
        "compare_cl.exact('Civico', 'Civico', label='Civico')\n",
        "compare_cl.exact('ZD', 'ZD', label='ZD')\n",
        "compare_cl.exact('Codice Via', 'Codice Via', label='Codice Via')\n",
        "compare_cl.exact('Settore Merceologico', 'Settore Merceologico', label='Settore Merceologico')\n",
        "compare_cl.string('Insegna', 'Insegna', method='jarowinkler', threshold=0.80, label='Insegna')\n",
        "compare_cl.numeric('Superficie vendita', 'Superficie vendita', method='linear', offset=15, label='Superficie vendita')\n",
        "compare_cl.numeric('Superficie altri usi', 'Superficie altri usi', method='linear', offset=15, label='Superficie altri usi')\n",
        "compare_cl.numeric('Superficie totale', 'Superficie totale', method='linear', offset=30, label='Superficie totale')\n",
        "features = compare_cl.compute(candidate_links, data1)\n",
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDmhq-ZIaC79"
      },
      "outputs": [],
      "source": [
        "# Count and sort the number of record pairs by their total similarity score\n",
        "features.sum(axis=1).value_counts().sort_index(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-o17DwuaJKy"
      },
      "outputs": [],
      "source": [
        "# compare the matches\n",
        "matches = features[features.sum(axis=1) > 7]\n",
        "\n",
        "print(len(matches))\n",
        "matches\n",
        "# Note: This method, when computed before the deduplication of rows, identifies the same pairs of the Blocking method, along with one additional pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2WbBcuxaTV7"
      },
      "outputs": [],
      "source": [
        "# Display the candidate duplicates\n",
        "data.loc[[555, 554]] # They are not duplicates because 'Civico' values are different"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbvnCBBmwUjc"
      },
      "source": [
        "# DATA QUALITY ASSESSMENT (again)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEtossppyxUD"
      },
      "source": [
        "COMPLETENESS :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zN4XXTRxSGG"
      },
      "outputs": [],
      "source": [
        "# total number of null values\n",
        "NULL = data.isnull().sum().sum()\n",
        "# total number of values in the dataset\n",
        "TOT = data.shape[0]*data.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHbjLp7-xdEU"
      },
      "outputs": [],
      "source": [
        "# completeness evaluation\n",
        "new_COMPLETENESS = '{0:.1f}%'.format((TOT - NULL) / TOT * 100)\n",
        "print(new_COMPLETENESS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBb0XJ6Jy6UE"
      },
      "source": [
        "ACCURACY of 'Superficie Totale' :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvooKeF4x2FF"
      },
      "outputs": [],
      "source": [
        "RANGE_INCORRECT = range(0,250) # range taken from the dataset documentation\n",
        "# number of out of bound values\n",
        "INCORRECT_Superficie_Totale = sum(1 for item in data['Superficie totale'] if item in RANGE_INCORRECT)\n",
        "# number of not null values\n",
        "NOT_NULL_Superficie_totale= data['Superficie totale'].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2cvrvcnyLIg"
      },
      "outputs": [],
      "source": [
        "# accuracy evaluation\n",
        "new_ACCURACY_SUPERFICIE_TOT = (data.shape[0]-INCORRECT_Superficie_Totale)/NOT_NULL_Superficie_totale\n",
        "new_ACCURACY_SUPERFICIE_TOT = '{0:.1f}%'.format(new_ACCURACY_SUPERFICIE_TOT*100)\n",
        "print(new_ACCURACY_SUPERFICIE_TOT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etomiFwmzkmg"
      },
      "source": [
        "CONSISTENCY :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnrNO-m7yeiX"
      },
      "outputs": [],
      "source": [
        "# integrity rule: superficie vendita + superficie altri usi = superficie totale\n",
        "data['consistency'] = np.where(data['Superficie vendita'] + data['Superficie altri usi'].fillna(0) != data['Superficie totale'], 0, 1)\n",
        "# number of rows that are consistent\n",
        "CONSISTENT = data['consistency'].sum()\n",
        "# count consistent rows\n",
        "COUNT = data['consistency'].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBA4AJDFyknf"
      },
      "outputs": [],
      "source": [
        "#cinsistency evaluation\n",
        "new_CONSISTENCY = CONSISTENT / COUNT\n",
        "new_CONSISTENCY = '{0:.1f}%'.format(new_CONSISTENCY * 100)\n",
        "print(new_CONSISTENCY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtYlp5nxyofF"
      },
      "outputs": [],
      "source": [
        "# drop the 'consistency' column\n",
        "data=data.drop(columns='consistency')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMvZkfbq0Emc"
      },
      "outputs": [],
      "source": [
        "# comparison of new and old DQ metrics\n",
        "print(\"COMPLETENESS\\t new: \" + new_COMPLETENESS + '\\t old: ' + COMPLETENESS)\n",
        "print(\"ACCURACY\\t new: \" + new_ACCURACY_SUPERFICIE_TOT + '\\t old: ' + ACCURACY_SUPERFICIE_TOT)\n",
        "print(\"CONSISTENCY\\t new: \" + new_CONSISTENCY + '\\t old: ' + CONSISTENCY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UASJx2v8sCPP"
      },
      "source": [
        "# 7) DATA ANALYSIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5pjCy0Xp2Yp"
      },
      "source": [
        "Classification on the cleaned dataset :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Pvd8x8ttTJ1"
      },
      "outputs": [],
      "source": [
        "## Remove useless columns\n",
        "# for the data analysis step, information about the address (which is equivalent to an ID) is useless\n",
        "data1= data.drop(columns=['Tipo Via', 'Via', 'Civico']).copy() # information about the address is still present in Codice Via and ZD.\n",
        "data1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQJtHzDRuMhJ"
      },
      "outputs": [],
      "source": [
        "#reset the index\n",
        "data1 = data1.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrL0k-T0ueEv"
      },
      "outputs": [],
      "source": [
        "#function to encode categorical values\n",
        "def encode(original_dataframe, feature_to_encode):\n",
        "    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]], dummy_na=True)\n",
        "    res = pd.concat([original_dataframe, dummies], axis=1)\n",
        "    res = res.drop([feature_to_encode], axis=1)\n",
        "    return (res)\n",
        "\n",
        "dataset = data1.copy()\n",
        "class_name = \"Settore Merceologico\"\n",
        "\n",
        "feature_cols = list(dataset.columns)\n",
        "feature_cols.remove(class_name)\n",
        "\n",
        "X = dataset[feature_cols] # Features\n",
        "y = dataset[class_name] # Target variable\n",
        "\n",
        "\n",
        "categorical_columns=list(X.select_dtypes(include=['bool','object']).columns)\n",
        "\n",
        "#encode categorical coluumns\n",
        "for col in X.columns:\n",
        "    if col in categorical_columns:\n",
        "        X = encode(X,col)\n",
        "\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZt74_2fyX-7"
      },
      "outputs": [],
      "source": [
        "X = StandardScaler().fit_transform(X) #scales numerical values from 0 to 1\n",
        "#X = np.nan_to_num(X) is useless because we already removed all nan\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyNgAFuw5rf4"
      },
      "outputs": [],
      "source": [
        "\n",
        "classifier = KNeighborsClassifier()\n",
        "\n",
        "print(\"Training the model...\")\n",
        "\n",
        "model_fit = classifier.fit(X, y)\n",
        "\n",
        "cv = ShuffleSplit(n_splits=2, test_size=0.2, random_state=2023)\n",
        "\n",
        "# Perform cross-validation and evaluate using accuracy and f1\n",
        "model_scores1 = cross_val_score(model_fit, X, y, cv=cv, scoring=\"accuracy\")\n",
        "model_scores2 = cross_val_score(model_fit, X, y, cv=cv, scoring=\"f1_macro\")\n",
        "\n",
        "accuracy_mean_clean = model_scores1.mean()\n",
        "f1_mean_clean = model_scores2.mean()\n",
        "\n",
        "print(\"Accuracy performance: \" + str(accuracy_mean_clean))\n",
        "print(\"F1 Score performance: \" + str(f1_mean_clean))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggHbqOcDsQ2M"
      },
      "source": [
        "same classification analysis with the dirty dataset :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZOZ9taXqI_f"
      },
      "outputs": [],
      "source": [
        "# load original dirty dataset\n",
        "original_data = pd.read_csv(\"https://raw.githubusercontent.com/SusannaPaoletti/DataQuality/refs/heads/main/Comune-di-Milano-Attivita-commerciali-di-media-e-grande-distribuzione.csv\", sep=';', encoding='utf-16')\n",
        "original_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYjYbwlJqD09"
      },
      "outputs": [],
      "source": [
        "dataset = original_data.drop(columns=['Ubicazione', 'Tipo Via', 'Via', 'Civico']).copy()\n",
        "class_name = \"Settore Merceologico\"\n",
        "\n",
        "feature_cols = list(dataset.columns)\n",
        "feature_cols.remove(class_name)\n",
        "\n",
        "X = dataset[feature_cols] # Features\n",
        "y = dataset[class_name] # Target variable\n",
        "\n",
        "\n",
        "# fill missing values of Settore Merceologico with mode\n",
        "y = y.fillna(y.mode()[0])\n",
        "\n",
        "\n",
        "categorical_columns=list(X.select_dtypes(include=['bool','object']).columns)\n",
        "\n",
        "#encode categorical columns\n",
        "for col in X.columns:\n",
        "    if col in categorical_columns:\n",
        "        X = encode(X,col)\n",
        "\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8qteefMqZ2m"
      },
      "outputs": [],
      "source": [
        "X = StandardScaler().fit_transform(X) #scales numerical values from 0 to 1\n",
        "X = np.nan_to_num(X)\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyfalmDBqhG1"
      },
      "outputs": [],
      "source": [
        "classifier = KNeighborsClassifier() #for categorical variable\n",
        "\n",
        "print(\"Training...\")\n",
        "\n",
        "model_fit = classifier.fit(X, y)\n",
        "\n",
        "cv = ShuffleSplit(n_splits=2, test_size=0.2, random_state=2023)\n",
        "\n",
        "# Perform cross-validation and evaluate using accuracy and f1\n",
        "model_scores1 = cross_val_score(model_fit, X, y, cv=cv, scoring=\"accuracy\")\n",
        "model_scores2 = cross_val_score(model_fit, X, y, cv=cv, scoring=\"f1_macro\")\n",
        "\n",
        "accuracy_mean_dirty = model_scores1.mean()\n",
        "f1_mean_dirty = model_scores2.mean()\n",
        "\n",
        "print(\"Accuracy performance: \" + str(accuracy_mean_dirty))\n",
        "print(\"F1 Score performance: \" + str(f1_mean_dirty))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2B2e4Vxzuaeg"
      },
      "outputs": [],
      "source": [
        "#f1 score comparison\n",
        "print(\"F1 metric of dirty dataset: \", f1_mean_dirty)\n",
        "print(\"F1 metric of clean dataset: \", f1_mean_clean)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save a DataFrame as a CSV\n",
        "data.to_csv('data_cleaned.csv', index=False)"
      ],
      "metadata": {
        "id": "7H8WIAEmmx_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hPeQc_I3mxqE"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8Sy2N0h6t620",
        "1ipXEWcQuSTd",
        "tOQTYKearj-R",
        "3f6VhkKlrq7-"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}